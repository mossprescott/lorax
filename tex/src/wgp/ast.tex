\section{Lorax 0.2}
\label{lorax}
To build programs directly out of nodes, we need a common representation for nodes. It must be flexible enough to represent many kinds of programs and support the kind of editing operations we will want to provide. It should provide a natural way to represent the primitive values which appear in programs, and the common ways of aggregating values into larger structures. It should be able to represent arbitrary programs, and allow nodes to be freely composed without imposing any fixed constraints.

Also, AST nodes will be called on to represent elements of not only source programs, but also intermediate programs, grammars, and graphical elements.

\subsection{Nodes, Values, and References}
A \emph{program} is a tree made up of \emph{nodes}. Every node has a \emph{type}, a unique \emph{label}, and a \emph{value}. A node's type identifies it as one of a class of related nodes, all of which have some common meaning (for example, the type \kw{plus} might represent addition expressions). A node's label is an opaque identifier which gives it a distinct identity, and allows references to nodes to survive transformations of the program. A node's value may be: \emph{empty}, if the node type alone carries all the node's meaning; an atomic value, which is a \emph{boolean}, \emph{integer}, or \emph{string} (a sequence of characters which are treated as an indivisible value); a \emph{sequence} of $n$ child nodes (indexed by integers $0$ to $n-1$) or an (unordered) \emph{map} of distinct \emph{attribute names} and associated child nodes. A \emph{reference} is a special type of node which has the type \kw{ref}, and has as its value the label of another node.

We define these four kinds of nodes because they seem to be sufficient to represent the elements of programs for the languages we experimented with. These elements seem to fall into four categories, corresponding to the four kinds of nodes. Of the four, map nodes are the most commonly used, and their ability to contain an arbitrary set of attributes is key to the extensibility of the system. Sequences are represented with a first-class node type primarily for the convenience of the editor, discussed later. Reference nodes represent any reference from one part of a program to an entity which is declared elsewhere, rendering such references unambiguous.

Nodes are immutable values, so a program is a \emph{persistent data structure} \cite{sarnak}. A modified program may be constructed by building a new tree sharing much of the structure of an existing tree, which allows an efficient system to be built out of many layers of tree transformation.

A program is \emph{well-formed} if its nodes satisfy the following constraints:
\begin{enumerate}
\item No two nodes have the same label.
\item No node appears as a child of more than one parent, or under more than one index/name of a sequence/map-valued node (i.e. the nodes form a tree).
\item The value of each reference node is the label of some node in the program.
\end{enumerate}

Note that any node can be considered as the root node of a sub-tree consisting of its descendant nodes. A sub-tree of a well-formed tree will be well-formed unless it contains a reference to a node which is not part of the same sub-tree. Such a reference is analogous to a free variable, and can play a similar role.

These minimal constraints allow trees to be transformed into one another using familiar functional programming techniques, and ensure that operations on trees have well-defined results. Therefore, \Meta\ requires that trees be well-formed at all times, and its components are designed to enforce these constraints.


\subsection{Specifications}
A \emph{specification} constrains the structure of nodes in a program. A program is \emph{valid} with respect to a certain specification if the arrangement of nodes, values, and references satisfies the specification's constraints.

In practice a program may contain violations of a specification, and the user will be interested in the nature of each violation in order to be able to fix them (by correcting either the program or the specification). Therefore a specification will typically be implemented in the form of a \emph{checker}, a function over programs that produces a set of \emph{errors} each consisting of a description of the problem and the location where it occurs.

Depending on the nature of the properties being checked, a specification may be defined only in terms of \emph{local} properties of individual nodes and their direct children, or may refer to \emph{non-local} relationships between nodes more distantly connected. In general, local properties are easier to define, easier to check, and easier to understand, so for the most part they are to be preferred. In section \ref{grammars}, we describe a particularly direct and convenient way of specifying these basic structural properties which is sufficient to define the abstract syntax of a programming language.

By providing a modular way of describing and checking properties of programs, specifications give much-needed structure to the open model of ASTs described in the preceding sections. They do not, however, restrict the user's ability to modify and extend her program and/or language, even if that means the program is invalid at times.


\subsection{Reductions}
\label{reduction}
The next step in making a useful system is providing a way to produce (multiple) \emph{target} programs from a given \emph{source} program. The central idea is to have a source program, consisting of nodes in some ``user'' programming language, which includes a node type for every important programming construct. In an extensible system, the user is able to add new types of nodes or even entire languages to a running system, even though the underlying system only understands a fixed set of node types. The way to bridge this gap is via \emph{reduction}.

Reduction is a restricted form of \emph{graph reduction}, inspired by the macro expansion process of Lisp.\footnote{The term ``Lisp'' is meant to refer to any of the many variants of Lisp, including Maclisp, Common Lisp, Emacs Lisp and Scheme.} In fact the term ``expansion'' might be more appropriate because a typical reduction replaces a more abstract node with a larger number of simpler nodes.

A \emph{source} program is reduced to a \emph{target} program by applying a \emph{reduction function} to the root node of the source program. If any reduction is possible, the reduction function returns a new, replacement root node, which typically repackages the children of the original root node under some new kind of parent. As long as some reduction is performed, the reduction function is applied repeatedly to the previous result. Eventually, the root node is fully-reduced (the function fails to return a new reduced node). At that point, each child node is reduced in the same way, and a new root node is constructed with the reduced children.

%Because each reduction step constructs an arbitrary replacement node, it's possible at least in theory to write any arbitrary transformation as a reduction. This means, for one thing, that reduction may not terminate. This generality, and the potential errors it allows, are typical of meta-programming systems; the full power of the language is available at compile-time, including the ability to introduce bad behavior. Although a system with less expressive power might be capable of meeting the need with less potential for bad behavior, in practice the kinds of problems that are encountered are most often easily fixed, provided that the system gives reasonably good error reports and no permanent damage is done when reductions do not perform as expected.

%During reduction a series of intermediate programs are produced which are partially reduced, and in general do not conform to the source or target specification. It might be interesting to investigate ways of defining specifications and reductions such that it could be statically shown that a valid source program always reduces to a valid target program. I have not investigated how this might work, but it certainly would require imposing restrictions on the results that could be produced by reductions, suggesting at the very least a static type system for the language of reductions.

%As a practical matter, it's convenient to define reduction functions only for properly-formed inputs. This can be facilitated by declaring both the specification (e.g. expected attributes for each node type) and the reduction (given a node with those attributes) at the same time. If the specification defines only local properties, then the reduction should assume only the presence of \emph{some} node at each required location, but make no demands on the form of these child nodes. 



%\subsection{Passing information down}
%A local reduction works by examining one node at a time, producing a reduced node which combines the children of the original node in a new way, possibly with some additional nodes inserted as well. This is often sufficient, but in some cases it's necessary to propagate some additional information through the reduction process. A simple extension that meets many needs is to augment the reduction function described above to accept an additional \emph{environment} parameter, an arbitrary value, and to return both a reduced node and a new environment value. When recursively visiting child nodes, the value resulting from the parent's reduction is used. This allows the reduction function to compute values based on the shape of the tree and use them to control some aspects of the reduction. For example, a reduction could keep track of the depth of the tree (distance from the root node).

%This simple approach works because the children of each node are always available to the reduction function, while the parent nodes are not. The ``inherited'' value allows the reduction function to accumulate some information about the otherwise inaccessible ancestors.

\subsection{Defining Specifications and Reductions}

\todo{A place to talk about defining transformations via attribute grammars and meta-language.}


\subsection{Characteristics}
This way of constructing program source has some implications for the way languages can be defined and the way programs can be worked with.

Node types, attributes, and specifications naturally fit with the concepts of \emph{tree grammars}, which allows specifications to be defined in ways that are familiar to language designers (i.e. with a grammar). Because grammars so-defined will be used only for checking tree structure and not for parsing, they can be constructed in the natural way, without any need for tricks to work around parsing algorithm shortcomings (e.g. left-factoring).

Programs are \emph{self-describing}. Each node carries an explicit declaration of its meaning, and each primitive value is manifestly of a certain type. This is in contrast to a textual language, where the same sequence of characters might represent a name, data, or a keyword, depending on the context in which they appear. This is a major advantage especially for tools that manipulate programs, because no parser is necessary to extract the structure of the program.

Labels provide robust \emph{source locations}. The label provides each node with an identity that survives when the other parts of the program are changed, or when the program is serialized to non-volatile storage, etc. Thus labels provide a way for tools (e.g. compilers and debuggers) to refer to source locations. For example, a typical refactoring operation such as changing the name of a variable or moving some code from one place to another looks like several unrelated changes to a \emph{diff} tool that has two versions of source text to look at, but a similar tool operating on labeled nodes could compare each \emph{node}, even if the location, the content, and even the type of the node have changed.

Reference nodes provide a way of referring to entities in a program which can never be ambiguous, and is independent of such language-specific notions as names and scope. Reference nodes require special support from editors, which may also take advantage of the explicit reference structure to provide enhanced presentation of references.

Programs may be serialized for storage, distributed processing, etc. For example, assuming a choice of character set and encoding, all the components of a node can be easily converted to a stream of characters. Because the representation permits only trees, each node can be serialized when it first occurs; there's no need for an encoding of back-references. Moreover, the choice of serialized format is not so important because the structure of programs is defined at the level of nodes. Any serialized form that preserves the meaning in the terms defined above is equally good.

\todo{Cut the rest of this section?} This representation of programs as nodes has some things in common with a handful of other tree-structured representations often used for source code and/or intermediate representations within tools such as compilers.

\subsubsection{Compared to XML}
XML and other related \emph{markup languages}, are designed to augment textual data with explicit structure for a variety of purposes. The structure of an XML document has much in common with the structure of nodes as defined here, and the \emph{XML Infoset}\cite{infoset} model for documents in particular has a similar flavor. However, there are some important differences:
\begin{enumerate}
\item The child elements of each node in an XML document are always in an ordered sequence, while named attributes can contain only simple character data. In \Meta, a node's children may be ordered or named, whichever makes sense, and there is no separate notion of attribute values vs. child nodes.
\item XML is explicitly a format for character streams, onto which an abstract model can be imposed \textit{post-hoc}. This leads to many awkward and ultimately uninteresting problems, such as when to ignore white-space and when it should be included in the data model. In \Meta, the source AST is defined in terms of the relevant types, and only nodes that actually contain character data are represented as characters.
\item Because XML is meant to be human-readable in a weak sense, text-editors are still the dominant mode of interaction with it. Although there are WYSIWYG tools for editing and viewing certain kinds of XML documents (e.g. SVG\cite{svg}, docx\cite{openoffice}), there are few general-purpose tools for working with XML per se, aside from text editors.
\end{enumerate}

XML's undesirability as a concrete syntax for programs has been well-established, and stands as an example of the usability challenges posed by representing source code as structured data \cite{holub}\cite{xml-bad-ant}. I believe this this failure largely results from the use of the textual representation as the editing interface. Although the XML syntax is particularly bad in this regard, any general-purpose, textual markup language will suffer in terms of readability for the flexibility it provides.

\subsubsection{Compared to S-expressions}
Most Lisp programs are written in \emph{s-expressions}, a simple data structure offering only lists, symbols (i.e. names), and a handful of types of primitive values. This simplicity gives considerable generality and flexibility, because it allows any number of new constructs to be introduced simply by defining new forms (macros). However, the use of s-expressions directly in lieu of a concrete syntax has been a highly divisive choice, essentially separating the population of programmers into two ``camps''. Only those who are not put off by sequences of nested parentheses can appreciate the expressive power that Lisp offers.

It is a major goal of this thesis to establish that the expressive power of the Lisp model can be realized in the context of a language that can be read by ``the rest of us.'' \Meta's representation for nodes has the same generality and free extensibility as s-expressions, and its simple and unambiguous structure makes it ideal for defining and transforming programs.

%\subsection{Compared to Algebraic Data Types}
%When language tools are written in functional languages such as Haskell and ML, nodes are often represented using \emph{algebraic data types}\cite{functionalAST}. Algebraic data types are rarely used directly to represent programs (because they are even more syntactically awkward than s-expressions), but are often used to represent programs after parsing. 

%The major difference is that these values are constrained by the type system to be valid. \Meta's nodes are subject to fewer hard restrictions, which allows programs to be freely modified by the programmer in the process of constructing and modifying the program.
